{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9cae3c98-a695-494b-9a18-ad9b35f8ad94",
      "metadata": {
        "id": "9cae3c98-a695-494b-9a18-ad9b35f8ad94"
      },
      "source": [
        "## Ques 1:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d9a50f9-2e02-4eb8-b521-7dc977e711ad",
      "metadata": {
        "id": "7d9a50f9-2e02-4eb8-b521-7dc977e711ad"
      },
      "source": [
        "### Ans: Pooling in Convolutional Neural Networks (CNNs) serves the following purposes and offers several benefits:\n",
        "### Purpose:\n",
        "### Down-sampling: Pooling reduces the spatial dimensions of the input feature maps, which helps in decreasing computational complexity and memory requirements.\n",
        "### Translation Invariance: Pooling provides a level of translation invariance, making the network less sensitive to small shifts in the input data.\n",
        "### Feature Invariance: Pooling captures essential features in a local neighborhood while discarding less important information, promoting feature invariance.\n",
        "### Benefits:\n",
        "### Computational Efficiency: Pooling reduces the number of computations in subsequent layers, making the network more computationally efficient.\n",
        "### Parameter Reduction: By down-sampling, pooling decreases the number of parameters in the network, helping to prevent overfitting and improving generalization.\n",
        "### Robustness: The translation invariance and feature invariance provided by pooling contribute to the robustness of the model, making it more effective in recognizing patterns in various orientations and positions.\n",
        "### Memory Efficiency: Smaller feature maps resulting from pooling require less memory, making it easier to train and deploy CNNs, especially in resource-constrained environments."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88173a4d-f4e9-4153-a1a1-f46d4e76b730",
      "metadata": {
        "id": "88173a4d-f4e9-4153-a1a1-f46d4e76b730"
      },
      "source": [
        "## Ques 2:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cf9c312-d47e-45e1-9d49-76842fefd081",
      "metadata": {
        "id": "2cf9c312-d47e-45e1-9d49-76842fefd081"
      },
      "source": [
        "### Ans: Max Pooling:\n",
        "### In max pooling, for each local region (typically a 2x2 or 3x3 window), the maximum value is retained.\n",
        "### The idea is to capture the most prominent feature in that region. Max pooling helps the network focus on the most activated features and discard less important information.\n",
        "### Max pooling provides a form of translation invariance and contributes to the network's robustness.\n",
        "### Min Pooling:\n",
        "### In min pooling, for each local region, the minimum value is retained.\n",
        "### Min pooling is less commonly used than max pooling. It may be employed in specific scenarios where capturing the minimum activation is relevant or for particular applications where lower values are more informative.\n",
        "### Min pooling can be more sensitive to noise compared to max pooling."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83c98db0-607b-4e55-adab-5fe168ac3bb6",
      "metadata": {
        "id": "83c98db0-607b-4e55-adab-5fe168ac3bb6"
      },
      "source": [
        "## Ques 3:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3fbc072-dea2-4e61-995d-7e9abb0a1c48",
      "metadata": {
        "id": "c3fbc072-dea2-4e61-995d-7e9abb0a1c48"
      },
      "source": [
        "### Ans: Padding in Convolutional Neural Networks (CNNs) is the process of adding extra pixels (usually zero-valued) around the input data before applying convolutional or pooling operations. The significance of padding lies in addressing issues related to the spatial dimensions of the feature maps and the information at the borders of the input.\n",
        "### Without padding, as convolutional or pooling operations are applied to the input data, the spatial dimensions of the feature maps reduce. This reduction can lead to the loss of information at the borders of the input.\n",
        "### Padding helps in preserving the information at the borders by adding extra pixels, ensuring that the convolutional or pooling operations can consider the entire input region.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0c3a1f9-2985-4475-843a-3c416e407d14",
      "metadata": {
        "id": "a0c3a1f9-2985-4475-843a-3c416e407d14"
      },
      "source": [
        "## Ques 4:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb20c85c-c162-4948-854c-e3a7c953f50b",
      "metadata": {
        "id": "eb20c85c-c162-4948-854c-e3a7c953f50b"
      },
      "source": [
        "### Ans: Zero padding and valid padding are two distinct strategies employed in Convolutional Neural Networks (CNNs) to handle the spatial dimensions of input data and subsequently influence the size of the output feature map. Zero padding involves augmenting the input with additional pixels, usually set to zero, which effectively increases the spatial dimensions of the input. Consequently, the output feature map is larger than the input, helping to prevent information loss at the borders and addressing the \"border effect.\" This strategy is particularly valuable when preserving details near the edges is critical. On the other hand, valid padding, also known as no padding, applies convolution or pooling operations without adding extra pixels around the input. This results in a reduction in the spatial dimensions of the input, and consequently, the output feature map is smaller than the input. While valid padding may lead to information loss at the borders, it is chosen when spatial reduction is acceptable or when computational efficiency is a priority.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a62fe859-de89-4097-b5f8-ec2f812e47b1",
      "metadata": {
        "id": "a62fe859-de89-4097-b5f8-ec2f812e47b1"
      },
      "source": [
        "## Ques 5:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f68b3378-0ab6-4fb9-958d-9039227443c3",
      "metadata": {
        "id": "f68b3378-0ab6-4fb9-958d-9039227443c3"
      },
      "source": [
        "### Ans: LeNet-5 is a pioneering Convolutional Neural Network (CNN) architecture developed by Yann LeCun and his colleagues in the 1990s. It was designed for handwritten digit recognition and is considered one of the earliest successful CNNs. LeNet-5 played a significant role in demonstrating the effectiveness of deep learning in computer vision tasks. Here's a brief overview of the LeNet-5 architecture:\n",
        "### Input Layer:\n",
        "### LeNet-5 takes as input grayscale images of size 32x32 pixels.\n",
        "### First Convolutional Layer (C1):\n",
        "### Convolution with a 5x5 kernel.\n",
        "### Output feature maps are subsampled (down-sampled) using average pooling with a 2x2 kernel.\n",
        "### Second Convolutional Layer (C3):\n",
        "### Convolution with a 5x5 kernel on the subsampled output of the first layer (C1).\n",
        "### Subsampling with average pooling.\n",
        "### Third Fully Connected Layer (F4):\n",
        "### A fully connected layer with 120 nodes.\n",
        "### Fourth Fully Connected Layer (F5):\n",
        "### Another fully connected layer with 84 nodes.\n",
        "### Output Layer:\n",
        "### The final output layer consists of 10 nodes, corresponding to the 10 possible digits (0-9).\n",
        "### Activation Function:\n",
        "### Sigmoid activation function is used in the hidden layers, and the output layer uses a softmax activation for multiclass classification.\n",
        "### Training Technique:\n",
        "### LeNet-5 utilizes a combination of convolutional layers and subsampling layers, followed by fully connected layers. It is trained using gradient-based optimization methods like stochastic gradient descent (SGD)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a9ef2f7-c1f8-4da0-b36a-20abd119a36a",
      "metadata": {
        "id": "3a9ef2f7-c1f8-4da0-b36a-20abd119a36a"
      },
      "source": [
        "## Ques 6:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1048c026-e78b-49d7-b4a2-2e896882bcd4",
      "metadata": {
        "id": "1048c026-e78b-49d7-b4a2-2e896882bcd4"
      },
      "source": [
        "### Ans: Input Layer:\n",
        "### Purpose: The input layer takes grayscale images of size 32x32 pixels as input. It serves as the starting point for processing the image data.\n",
        "### First Convolutional Layer (C1):\n",
        "### Purpose: This layer performs convolution operations on the input image using a 5x5 kernel. The output feature maps capture low-level features such as edges and corners. Subsequently, average pooling with a 2x2 kernel is applied, leading to down-sampling and feature reduction.\n",
        "### Second Convolutional Layer (C3):\n",
        "### Purpose: Building on the output of the first layer, C3 performs another set of convolution operations using a 5x5 kernel. Similar to the first layer, average pooling is applied for down-sampling. This layer captures higher-level features based on the patterns detected in the first layer.\n",
        "### Third Fully Connected Layer (F4):\n",
        "### Purpose: F4 is a fully connected layer with 120 nodes. It takes the output of the previous layers and transforms it into a lower-dimensional representation, facilitating feature learning and abstraction.\n",
        "### Fourth Fully Connected Layer (F5):\n",
        "### Purpose: F5 is another fully connected layer with 84 nodes. It further refines the learned features from the previous layers, preparing the network for the final classification.\n",
        "### Output Layer:\n",
        "### Purpose: The output layer consists of 10 nodes, each representing a digit from 0 to 9. It employs the softmax activation function to convert the raw output into probabilities, facilitating multiclass classification.\n",
        "### Activation Function (Sigmoid and Softmax):\n",
        "### Purpose: Sigmoid activation functions are used in the hidden layers to introduce non-linearity, allowing the network to learn complex representations. The softmax activation function in the output layer converts the raw scores into probabilities, aiding in the classification of digits.\n",
        "### Training Techniques:\n",
        "### Purpose: LeNet-5 is trained using gradient-based optimization methods such as stochastic gradient descent (SGD). The training process involves adjusting the weights and biases to minimize the error between predicted and actual class labels."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e566a02-5c5e-481e-ad74-40999ddfa39f",
      "metadata": {
        "id": "1e566a02-5c5e-481e-ad74-40999ddfa39f"
      },
      "source": [
        "## Ques 7:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a8ab896-64d9-497f-a7f6-7e85814cc200",
      "metadata": {
        "id": "3a8ab896-64d9-497f-a7f6-7e85814cc200"
      },
      "source": [
        "### Ans: Advantages of LeNet-5:\n",
        "### Pioneering Success: LeNet-5 was a groundbreaking architecture that demonstrated the effectiveness of convolutional neural networks (CNNs) for image classification tasks. It was one of the earliest models to successfully learn hierarchical features from images.\n",
        "### Hierarchical Feature Learning: The architecture of LeNet-5 includes convolutional layers followed by subsampling layers, allowing the network to automatically learn relevant features at different levels of abstraction. This hierarchical feature learning is a fundamental concept in modern CNNs.\n",
        "### Small and Efficient: LeNet-5 has a relatively small number of parameters compared to contemporary models, making it computationally efficient. This was advantageous in the era when computational resources were more limited.\n",
        "### Applications in Handwriting Recognition: LeNet-5 was initially designed for handwritten digit recognition, and it excelled in this task. Its success laid the groundwork for the application of CNNs in various image recognition and classification tasks.\n",
        "### Limitations of LeNet-5:\n",
        "### Limited Capacity for Complex Tasks: The architecture of LeNet-5 is relatively shallow compared to modern CNNs. While it performed well for handwritten digit recognition, it may not have the capacity to handle more complex image classification tasks with intricate features and variations.\n",
        "### Fixed Input Size: LeNet-5 is designed for fixed-size input images (32x32 pixels). Handling images of different sizes requires resizing, potentially leading to information loss or distortion.\n",
        "### Sigmoid Activation: LeNet-5 uses the sigmoid activation function, which has limitations, such as vanishing gradient problems, hindering the training of very deep networks. Modern architectures often use Rectified Linear Unit (ReLU) activations for better performance.\n",
        "### Lack of Batch Normalization: LeNet-5 predates the introduction of batch normalization, a technique that helps stabilize and accelerate training in deep networks. The absence of this technique may result in longer training times and convergence challenges for deeper architectures.\n",
        "### Not Suitable for Large Datasets: LeNet-5 was designed and trained on relatively small datasets compared to contemporary datasets. It may not generalize well to larger and more diverse datasets without modifications or adaptations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7dec9061-108c-4287-b797-e090c69eb077",
      "metadata": {
        "id": "7dec9061-108c-4287-b797-e090c69eb077"
      },
      "source": [
        "## Ques 8:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41d5fdc4-e205-4525-9f47-4cc78defc4f7",
      "metadata": {
        "id": "41d5fdc4-e205-4525-9f47-4cc78defc4f7"
      },
      "source": [
        "### Ans:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Load and preprocess the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize pixel values to be between 0 and 1\n",
        "\n",
        "# Reshape the data to fit LeNet-5 architecture (32x32 input)\n",
        "x_train = x_train.reshape((-1, 28, 28, 1))\n",
        "x_test = x_test.reshape((-1, 28, 28, 1))\n",
        "\n",
        "# LeNet-5 architecture\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(6, (5, 5), activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(16, (5, 5), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(120, activation='relu'))\n",
        "model.add(layers.Dense(84, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy: {test_acc}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJweyPYE1DGv",
        "outputId": "982f433e-49a3-4a0e-a0e9-6120a99086f0"
      },
      "id": "AJweyPYE1DGv",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 37s 19ms/step - loss: 0.2000 - accuracy: 0.9376 - val_loss: 0.0663 - val_accuracy: 0.9793\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 33s 17ms/step - loss: 0.0672 - accuracy: 0.9788 - val_loss: 0.0544 - val_accuracy: 0.9827\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 34s 18ms/step - loss: 0.0482 - accuracy: 0.9843 - val_loss: 0.0449 - val_accuracy: 0.9863\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 36s 19ms/step - loss: 0.0402 - accuracy: 0.9869 - val_loss: 0.0468 - val_accuracy: 0.9846\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 34s 18ms/step - loss: 0.0327 - accuracy: 0.9897 - val_loss: 0.0482 - val_accuracy: 0.9854\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 34s 18ms/step - loss: 0.0280 - accuracy: 0.9910 - val_loss: 0.0419 - val_accuracy: 0.9881\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 35s 19ms/step - loss: 0.0233 - accuracy: 0.9922 - val_loss: 0.0358 - val_accuracy: 0.9896\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 35s 19ms/step - loss: 0.0211 - accuracy: 0.9933 - val_loss: 0.0350 - val_accuracy: 0.9901\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 36s 19ms/step - loss: 0.0187 - accuracy: 0.9942 - val_loss: 0.0459 - val_accuracy: 0.9877\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 35s 19ms/step - loss: 0.0162 - accuracy: 0.9944 - val_loss: 0.0391 - val_accuracy: 0.9897\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0391 - accuracy: 0.9897\n",
            "Test accuracy: 0.9897000193595886\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a5eb113-969e-4431-a25e-ed93dc81aec1",
      "metadata": {
        "id": "0a5eb113-969e-4431-a25e-ed93dc81aec1"
      },
      "source": [
        "## Ques 9:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7961e41-2bdb-4b39-adc1-09378b24e2ef",
      "metadata": {
        "id": "b7961e41-2bdb-4b39-adc1-09378b24e2ef"
      },
      "source": [
        "### Ans: AlexNet is a landmark convolutional neural network (CNN) architecture that played a crucial role in the advancement of deep learning, particularly in image classification tasks. Developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, AlexNet won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012, significantly outperforming traditional computer vision methods. Here's a brief overview of the AlexNet architecture:\n",
        "### Input Layer:\n",
        "### The network takes as input color images with dimensions 227x227 pixels.\n",
        "### Convolutional Layers (Conv1-Conv5):\n",
        "### The architecture comprises five convolutional layers. The first convolutional layer (Conv1) applies a 11x11 filter with a large stride, extracting low-level features.\n",
        "### Subsequent convolutional layers (Conv2-Conv5) use smaller filter sizes (3x3 and 5x5) to capture increasingly complex features.\n",
        "### Convolutional layers are followed by rectified linear unit (ReLU) activation functions to introduce non-linearity.\n",
        "### Max Pooling Layers (Pool1-Pool3):\n",
        "### Between the convolutional layers, max pooling layers are applied (Pool1-Pool3), reducing spatial dimensions and providing a degree of translation invariance.\n",
        "### Local Response Normalization (LRN):\n",
        "### LRN is applied after the first and second convolutional layers to normalize responses and enhance the network's generalization capabilities.\n",
        "### Fully Connected Layers (FC6-FC8):\n",
        "### Three fully connected layers (FC6-FC8) follow the convolutional and pooling layers.\n",
        "### FC6 and FC7 serve as feature extractors, mapping high-level features from the convolutional layers.\n",
        "### FC8 is the output layer with 1000 nodes, representing the 1000 ImageNet classes.\n",
        "### Dropout:\n",
        "### Dropout is applied to the fully connected layers (FC6 and FC7) to prevent overfitting during training by randomly deactivating some neurons.\n",
        "### Softmax Activation:\n",
        "### The output layer uses softmax activation to convert the network's raw scores into probability distributions over the 1000 ImageNet classes.\n",
        "### Training Techniques:\n",
        "### AlexNet was trained using stochastic gradient descent (SGD) with momentum. Data augmentation, such as image flipping and cropping, was employed to improve generalization."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f61408cb-a031-4886-a312-dc4e37901f39",
      "metadata": {
        "id": "f61408cb-a031-4886-a312-dc4e37901f39"
      },
      "source": [
        "## Ques 10:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "484bbce3-fde5-4f24-a1dd-69ec27f4ac0e",
      "metadata": {
        "id": "484bbce3-fde5-4f24-a1dd-69ec27f4ac0e"
      },
      "source": [
        "### Ans: Deep Architecture:\n",
        "### AlexNet was one of the first deep convolutional neural networks (CNNs) with a substantial depth. It comprised eight layers, including five convolutional layers and three fully connected layers. This depth allowed the model to learn hierarchical representations of features, capturing both low-level and high-level information.\n",
        "### ReLU Activation Function:\n",
        "### AlexNet used the rectified linear unit (ReLU) activation function instead of traditional activation functions like sigmoid or hyperbolic tangent. ReLU introduces non-linearity to the model and helps alleviate the vanishing gradient problem, enabling faster and more effective training of deep networks.\n",
        "### Local Response Normalization (LRN):\n",
        "### LRN was applied after the first and second convolutional layers. This normalization technique enhances the model's generalization by normalizing the responses within local regions, promoting competition among adjacent neurons and improving feature discrimination.\n",
        "### Overlapping Max Pooling:\n",
        "### AlexNet utilized overlapping max pooling, where the pooling regions overlap, in contrast to traditional non-overlapping pooling. Overlapping pooling reduces spatial resolution less aggressively and helps preserve more spatial information, contributing to better performance.\n",
        "### Dropout Regularization:\n",
        "### Dropout was introduced in the fully connected layers (FC6 and FC7) to prevent overfitting. During training, dropout randomly deactivates a fraction of neurons, forcing the network to learn more robust and generalized features.\n",
        "### Data Augmentation:\n",
        "### To further improve generalization, AlexNet employed data augmentation techniques such as image flipping and cropping during training. This artificially increased the size of the training dataset and helped the model become more invariant to variations in input data.\n",
        "### Large Convolutional Kernels:\n",
        "### The first convolutional layer (Conv1) used a large 11x11 filter size with a stride of 4. This choice allowed the network to capture larger receptive fields, extracting low-level features efficiently.\n",
        "### Parallelization and GPU Usage:\n",
        "### AlexNet was designed to take advantage of GPU acceleration. The parallelization of operations across multiple GPUs significantly accelerated training times, enabling the efficient training of deeper neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f3f89b8-49b8-41ce-9416-be8db1a521ab",
      "metadata": {
        "id": "9f3f89b8-49b8-41ce-9416-be8db1a521ab"
      },
      "source": [
        "## Ques 11:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69e5bb13-18ab-4df5-8a28-78a494aa898b",
      "metadata": {
        "id": "69e5bb13-18ab-4df5-8a28-78a494aa898b"
      },
      "source": [
        "### Ans: Convolutional Layers:\n",
        "### Role: Convolutional layers in AlexNet play a fundamental role in feature extraction. They apply convolutional operations to input images using learnable filters or kernels. These operations enable the network to detect various hierarchical features, such as edges, textures, and patterns, in different spatial locations. The convolutional layers capture local features in the input images and progressively learn more complex and abstract representations as the depth of the network increases.\n",
        "### Pooling Layers:\n",
        "### Role: Pooling layers, specifically max pooling in the case of AlexNet, are interleaved with convolutional layers. These layers serve to down-sample the spatial dimensions of the feature maps, reducing the computational load and the risk of overfitting. Max pooling retains the most activated features within local regions, providing a form of translation invariance. Overlapping pooling regions in AlexNet contribute to preserving spatial information more effectively compared to traditional non-overlapping pooling. Pooling layers contribute to the network's ability to capture hierarchical features and make the model more robust.\n",
        "### Fully Connected Layers:\n",
        "### Role: The fully connected layers in AlexNet, particularly FC6 and FC7, serve as feature extractors and high-level representation learners. These layers take the flattened output of the preceding convolutional and pooling layers and map it to lower-dimensional representations. The fully connected layers capture global dependencies and relationships among features, providing the model with the ability to make high-level semantic interpretations. The final fully connected layer, FC8, corresponds to the output layer, where the network produces class probabilities for the given input image. The softmax activation function is applied to generate a probability distribution across the output classes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5d18c5a-13af-4c51-8ee8-ff4e9f0f9b1f",
      "metadata": {
        "id": "d5d18c5a-13af-4c51-8ee8-ff4e9f0f9b1f"
      },
      "source": [
        "## Ques 12:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33eef50e-7699-4049-a367-bc040c075252",
      "metadata": {
        "id": "33eef50e-7699-4049-a367-bc040c075252"
      },
      "source": [
        "### Ans:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, datasets\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "P5_Svu5h2G36"
      },
      "id": "P5_Svu5h2G36",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgEr-fC83FXD",
        "outputId": "027915a8-0dc7-47fe-f9c8-28306d56df10"
      },
      "id": "xgEr-fC83FXD",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 4s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(96, (11, 11), strides=(4, 4), activation='relu', input_shape=(32, 32, 3)))\n",
        "model.add(layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "model.add(layers.Conv2D(256, (5, 5), padding='same', activation='relu'))\n",
        "# model.add(layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "model.add(layers.Conv2D(384, (3, 3), padding='same', activation='relu'))\n",
        "model.add(layers.Conv2D(384, (3, 3), padding='same', activation='relu'))\n",
        "model.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "# model.add(layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))"
      ],
      "metadata": {
        "id": "oacrhPBu3IgD"
      },
      "id": "oacrhPBu3IgD",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(4096, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(4096, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(10, activation='softmax'))"
      ],
      "metadata": {
        "id": "8DhsqDwt33iv"
      },
      "id": "8DhsqDwt33iv",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "yGFmBofO4bG-"
      },
      "id": "yGFmBofO4bG-",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "datagen.fit(x_train)"
      ],
      "metadata": {
        "id": "vuYBRqLB4eNd"
      },
      "id": "vuYBRqLB4eNd",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(datagen.flow(x_train, y_train, batch_size=64),\n",
        "          steps_per_epoch=len(x_train) / 64, epochs=20, validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FS0kIolU4grc",
        "outputId": "29d441c2-dd70-4c33-8f5d-4b46d8cdb83c"
      },
      "id": "FS0kIolU4grc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "781/781 [==============================] - 1028s 1s/step - loss: 2.3034 - accuracy: 0.1206 - val_loss: 2.3028 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/20\n",
            "781/781 [==============================] - 1027s 1s/step - loss: 2.3029 - accuracy: 0.0625 - val_loss: 2.3026 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/20\n",
            "781/781 [==============================] - 1030s 1s/step - loss: 2.3028 - accuracy: 0.0631 - val_loss: 2.3027 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/20\n",
            "781/781 [==============================] - 1029s 1s/step - loss: 2.3028 - accuracy: 0.0759 - val_loss: 2.3026 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/20\n",
            "781/781 [==============================] - 1026s 1s/step - loss: 2.3027 - accuracy: 0.0305 - val_loss: 2.3026 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/20\n",
            "781/781 [==============================] - 1027s 1s/step - loss: 2.3028 - accuracy: 0.3102 - val_loss: 2.3027 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/20\n",
            "781/781 [==============================] - 1028s 1s/step - loss: 2.3028 - accuracy: 0.0000e+00 - val_loss: 2.3026 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/20\n",
            "781/781 [==============================] - 1035s 1s/step - loss: 2.3027 - accuracy: 0.1742 - val_loss: 2.3026 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/20\n",
            "781/781 [==============================] - 1032s 1s/step - loss: 2.3027 - accuracy: 0.1347 - val_loss: 2.3026 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/20\n",
            "408/781 [==============>...............] - ETA: 8:24 - loss: 2.3026 - accuracy: 0.0000e+00"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy: {test_acc}')"
      ],
      "metadata": {
        "id": "1O6fVJX54jWE"
      },
      "id": "1O6fVJX54jWE",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}